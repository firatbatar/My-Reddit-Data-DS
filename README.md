# My Reddit Usage Data Analysis

## Description

Sabanci University CS210 Introduction to Data Science Course Fall 2023-2024 Term Project.  
This project will be an analysis on my own <a href="https://www.reddit.com/" target="_blank">Reddit</a> usage.

## Motivation

While thinking about a project I thought I might analyze my interaction with the outside world which is done mostly through social media. Since Reddit is my primary social media platform, I decided to go with that.  
The motivation of this analysis is to gain a deeper understanding of my own Reddit usage patterns. By analyzing the subreddits I am subscribed to, when I am active on Reddit, and my engagement with posts and comments, I aim to gain insights into my online behavior and interests.

### Tools

**[Jupyter Notebook](https://jupyter.org/):** Used for coding and documentation.  
**[Pandas](https://pandas.pydata.org/):** Used for data cleaning, filtering, and structuring.  
**[Altair](https://altair-viz.github.io/index.html):** Mainly employed for exploratory data analysis (EDA) and interactive visualizations.  
**[Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/):** Supplementary for visualizations if needed.  
**[Numpy](https://numpy.org/):** Used for mathematical operations.  
**[Scipy](https://www.scipy.org/):** Used for statistical analysis.  
**[distfit](https://github.com/erdogant/distfit)** Used for distribution fitting.

## Data Source

I have two main sources of data:

-   Data that is directly exported from Reddit as a personal [data request](https://www.reddit.com/settings/data-request).
-   Data that is scrapped using [Reddit API](https://www.reddit.com/dev/api/) through a simple Python script using [PRAW](https://github.com/praw-dev/praw) package, following [Reddit's API rules](https://github.com/reddit/reddit/wiki/API). Script is available in `reddit_api_scraping.py` file.

In addition to these two, in order to create a general content type system for subreddits, I have created a Google Form to get "tags" annotated by hand. I wrote a Google Apps Script to automatically create a Google Form automatically from the data. Form is available [here](https://forms.gle/cKnMTpAyFxGkHKVy5). I have collected **7** responses from my friends. I have basically selected the most common tags three tags for each subreddit, but I have applied some more filtering. For details, see `reddit_api_scraping.py` file.

### Requested data

Requested data is in .csv format and contains:

-   Subscribed subreddits
-   Login history
-   Voted posts and comments
-   Created posts and comments

### Reddit API

Using Reddit API, I scrapped the following data:

-   The score (net upvotes) for each post and comment that I have voted.
-   The score of each post and comment that I have created.
-   The same script also combines annotated data with the raw data.

Requested data is read by scraper script and after scrapping, it is saved as new .csv files.  

Then the data is processed in `data_process.ipynb` file. The process includes cleaning and structuring the data. For details on every step for each different group of data, see the mentioned file.  

### Data Processing

The requested data and the scrapped data include many unused information and also some information that needs to be preprocessed. For all the details on the data processing, see `data_process.ipynb` file. But here is a summary of the process:  

-  **Subreddits:** This data also holds followed users, but I did not need them.  
- **Login History:** The date is in UTC, so I converted it to my local time and extracted the day name. Also IP addresses are not needed.  
- **Voted Posts and Comments:** The data includes some URLs about the objects, I have extracted the subreddit names from them, but I do not need the URLs themselves. Also, I have compared this data with subscribed subreddits and created a new column to indicate if the subreddit is subscribed or not.  
- **Created Posts and Comments:** This also includes some URLs, I have extracted the subreddit names from them and dropped the URLs. Again I have checked if the subreddit is subscribed or not. Also dropped the unnecessary columns.  

### Data Visualizations

Almost all the visualizations, the ones that are specific to an analysis are not here, are made in `data_visualization.ipynb` file. During the EDA, I have created many visualizations to understand the data better and to come up with interesting questions. Not all the visualizations are used and included in the final report. However, all can be found in the notebook and also in the [figures folder](/figures/img/).  

I have used Altair for most of the visualizations. Altair is a declarative statistical visualization library for Python, based on Vega. I used Altair because it is very easy to use and very powerful, I wanted to create all my visualizations interactive so that EDA and final report is more clear and interesting.

### Data Analysis

All the analysis is done in `data_analysis.ipynb` file. Notebook is structured and commented in a way that it can be followed easily. In addition to the statistical tests, the notebook also includes some of the visualizations that are used in the report. Again (almost) all the visualizations are made using Altair so they are interactive.

The details of the analysis can be found in the notebook, but the final report is in its [own website](https://firatbatar.com/my-reddit-usage).

## Findings

_For the detailed findings, please visit the [report](https://firatbatar.com/my-reddit-usage)._

Through this analysis, I discovered:  
- Patterns in my Reddit usage times and correlations with my class schedule.
- Voting patterns on posts/comments, including preferences based on subreddits and their tags.
- Self-reflection on my own contributions through posts/comments and their respective subreddits and tags.
- The factors that affect the way I interact on Reddit.
- How I start, join, and contribute to discussions on Reddit.

## Limitations

The limitations can be generally separated into two categories:

### Data Sourced Limitations

**Data Completeness:** Considering my time on Reddit and the way I interact with data is somewhat limited, the data stays a bit short for some of the possible analysis.  
**Subjectivity:** Like I have stated, Reddit does not have a native system to classify subreddit. Even though I tried to solve this using human annotators, they may have subjective biases affecting the accuracy of subreddit tags.

### Personal Limitations

**Privacy:** Since the project is based on a personal data, I have to be careful about the privacy of the data. There were some details that I did not want to share, so I did not use them in the analysis.  
**Knowledge:** Especially in the first stages of the project, I did not have enough knowledge about data science and data analysis. So, even though I tried to improve the project and the analysis as I learned more, there are still some parts inherited from the early stages of the project and some parts that I could not improve. With more knowledge and experience, and some knowledge about machine learning, I believe the project can be improved.

## Future Work

**Longitudinal Analysis:** Since data is on my usage of a social media platform, the analysis can be updated with newer data over a more extended period to observe changes in behavior and interests over time.  
**Advanced Analysis Techniques:** With more knowledge and experience on data science and machine learning, if I continue to take courses on the subject, I might return and improve some things that I am not able to do now.  
**Final Report Website:** I have created a website for the final report. However, due to my limited knowledge especially on using Altair, I could not create the website as detailed as I wanted and combine with interactive visualizations. I believe there is some room for improvement there.  